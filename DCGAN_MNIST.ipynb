{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled49.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbMwE+RCrWlpNS680/WoN0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/pytorch-models/blob/master/DCGAN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n7_YJvqjMH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "import statistics\n",
        "\n",
        "def load_datasets():\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))]\n",
        "    )\n",
        "    trainset = torchvision.datasets.MNIST(root=\"./data\",\n",
        "                                          train=True, download=True,\n",
        "                                          transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=512,\n",
        "                                               shuffle=True, num_workers=4)\n",
        "    return train_loader\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 64, 4, 1, 0), # 4x4\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 4, 1, 0), #7x7\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, 2, 2, 0), #14x14\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 1, 2, 2, 0), #28x28\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.AvgPool2d(7),\n",
        "            nn.Conv2d(64, 1, 1) # fcの代わり\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).squeeze()\n",
        "\n",
        "\n",
        "def train():\n",
        "    # モデル\n",
        "    device = \"cuda\"\n",
        "    model_G, model_D = Generator(), Discriminator()\n",
        "    model_G, model_D = nn.DataParallel(model_G), nn.DataParallel(model_D)\n",
        "    model_G, model_D = model_G.to(device), model_D.to(device)\n",
        "\n",
        "    params_G = torch.optim.Adam(model_G.parameters(),\n",
        "                lr=0.0002, betas=(0.5, 0.999))\n",
        "    params_D = torch.optim.Adam(model_D.parameters(),\n",
        "                lr=0.0002, betas=(0.5, 0.999))\n",
        "    \n",
        "    # ロスを計算するためのラベル変数\n",
        "    ones = torch.ones(512).to(device)\n",
        "    zeros = torch.zeros(512).to(device)\n",
        "    loss_f = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # エラー推移\n",
        "    result = {}\n",
        "    result[\"log_loss_G\"] = []\n",
        "    result[\"log_loss_D\"] = []\n",
        "\n",
        "    # 訓練\n",
        "    for i in range(100):\n",
        "        log_loss_G, log_loss_D = [], []\n",
        "        for real_img, _ in tqdm(load_datasets()):\n",
        "            batch_len = len(real_img)\n",
        "\n",
        "            # Gの訓練\n",
        "            # 偽画像を作成\n",
        "            z = torch.randn(batch_len, 64, 1, 1).to(device)\n",
        "            fake_img = model_G(z)\n",
        "\n",
        "            # 偽画像を一時保存\n",
        "            fake_img_tensor = fake_img.detach()\n",
        "\n",
        "            # 偽画像を本物と騙せるようにロスを計算\n",
        "            out = model_D(fake_img)\n",
        "            loss_G = loss_f(out, ones[:batch_len])\n",
        "            log_loss_G.append(loss_G.item())\n",
        "\n",
        "            # 微分計算・重み更新\n",
        "            params_D.zero_grad()\n",
        "            params_G.zero_grad()\n",
        "            loss_G.backward()\n",
        "            params_G.step()\n",
        "\n",
        "            # Discriminatoの訓練\n",
        "            # sample_dataの実画像\n",
        "            real_img = real_img.to(device)\n",
        "            # 実画像を実画像と識別できるようにロスを計算\n",
        "            real_out = model_D(real_img)\n",
        "            loss_D_real = loss_f(real_out, ones[:batch_len])\n",
        "\n",
        "            # 偽の画像の偽と識別できるようにロスを計算\n",
        "            fake_out = model_D(fake_img_tensor)\n",
        "            loss_D_fake = loss_f(fake_out, zeros[:batch_len])\n",
        "\n",
        "            # 実画像と偽画像のロスを合計\n",
        "            loss_D = loss_D_real + loss_D_fake\n",
        "            log_loss_D.append(loss_D.item())\n",
        "\n",
        "            # 微分計算・重み更新\n",
        "            params_D.zero_grad()\n",
        "            params_G.zero_grad()\n",
        "            loss_D.backward()\n",
        "            params_D.step()\n",
        "\n",
        "        result[\"log_loss_G\"].append(statistics.mean(log_loss_G))\n",
        "        result[\"log_loss_D\"].append(statistics.mean(log_loss_D))\n",
        "        print(\"log_loss_G =\", result[\"log_loss_G\"][-1], \", log_loss_D =\", result[\"log_loss_D\"][-1])\n",
        "\n",
        "        # 画像を保存\n",
        "        if not os.path.exists(\"mnist_generated\"):\n",
        "            os.mkdir(\"mnist_generated\")\n",
        "        torchvision.utils.save_image(fake_img_tensor[:min(batch_len, 100)],\n",
        "                                f\"mnist_generated/epoch_{i:03}.png\")\n",
        "    # ログの保存\n",
        "    with open(\"mnist_generated/logs.pkl\", \"wb\") as fp:\n",
        "        pickle.dump(result, fp)\n",
        "\n",
        "def save_test():\n",
        "    for X, _ in load_datasets():\n",
        "        torchvision.utils.save_image(X[:100], \"mnist_generated/true.png\")\n",
        "        break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}